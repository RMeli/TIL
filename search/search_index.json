{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer"]},"docs":[{"location":"","title":"Rocco Meli's TIL","text":"<p>Today I Learned: this is a collection of things I've learned in my day-to-day work. It's a mix of code snippets, tricks, and tips that I've picked up over the years. Some might be ovious, but they are here as a quick reference.  Others might be less obvious and more interesting.</p>"},{"location":"#useful-tools","title":"Useful tools","text":""},{"location":"#work","title":"Work","text":"<ul> <li>direnv: environment switcher</li> <li>tmux: terminal multiplexer</li> </ul>"},{"location":"#working-with-files","title":"Working with files","text":"<ul> <li>bat: <code>cat</code> clone</li> <li>diff-so-fancy: diff enhancer</li> <li>eza: <code>ls</code> clone</li> <li>fzf: fuzzy finder</li> <li>jq: command-line JSON processor</li> <li>nnn: terminal file manager</li> </ul>"},{"location":"#source-control","title":"Source control","text":"<ul> <li>git: distributed version control system</li> <li>gh: GitHub CLI</li> </ul>"},{"location":"#monitoring","title":"Monitoring","text":"<ul> <li>duf: disk usage utility</li> <li>btop: resource monitor</li> </ul>"},{"location":"cheminformatics/openbabel/","title":"Open Babel","text":""},{"location":"cheminformatics/openbabel/#gzipped-files","title":"GZipped Files","text":"<p>Open Babel deals with GZipped input files automatically, whether the <code>.gz</code> extension is present or not.</p> <p>For writing files, however, the <code>.gz</code> extension is not enough, and the <code>-z</code> option need to be used explicitly:</p> <pre><code>obabel -i&lt;INPUT_EXTENSION&gt; &lt;INPUT_FILE&gt; -o&lt;OUTPUT_EXTENSION&gt; -O &lt;OUTPUT_FILE&gt; -z\n</code></pre> <p><code>&lt;OUTPUT_FILE&gt;</code> needs to specify the <code>.gz</code> extension explicitly, since it is not appended automatically.</p>"},{"location":"cheminformatics/rdkit/","title":"RDKit","text":""},{"location":"cheminformatics/rdkit/#conformers-generation","title":"Conformers Generation","text":"<pre><code>mol = Chem.MolFromSmiles(smi)\n\nmol = Chem.AddHs(mol)\n\nparams = AllChem.ETKDGv2() # Riniker &amp; Landrum, 2015\nparams.randomSeed = 42\n\nAllChem.EmbedMultipleConfs(mol, numConfs=1, params=params)\n</code></pre>"},{"location":"cheminformatics/rdkit/#imports","title":"Imports","text":"<pre><code>from rdkit import Chem\nfrom rdkit.Chem import AllChem\n</code></pre>"},{"location":"containers/containers/","title":"Containers","text":""},{"location":"containers/containers/#install-missing-packages","title":"Install missing packages","text":"<p>If some packages are missing from the container, it is possible to install them with the following workaround:</p> <pre><code>apt update &amp;&amp; apt download &lt;PACKAGE&gt; # (1)!\ndpkg -x &lt;PACKAGE&gt;.deb / # (2)!\n</code></pre> <ol> <li>Download the <code>.deb</code> package file for <code>&lt;PACKAGE&gt;</code>.</li> <li>Extract the contents of the package into the container.</li> </ol> <p>This is useful in situations where <code>apt install</code> can't be used.</p>"},{"location":"containers/containers/#podman","title":"podman","text":"<ul> <li>podman</li> <li>NVIDIA Container Toolkit</li> </ul>"},{"location":"containers/containers/#gpu-support","title":"GPU support","text":"<p>The NVIDIA Container Toolkit <code>nvidia-ctk</code> needs to be installed. In order to run a <code>podman</code> container with GPU support, one needs to generate a CDI specification.</p> <pre><code>sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\nnvidia-ctk cid list\u001c\n</code></pre> Container Device Interface <p>CDI is an open specification for container runtimes that abstracts what access to a device means, and standardizes access across container runtimes.</p> <p>To run a <code>podman</code> container with GPU support, use the following:</p> <pre><code>podman run --device=nvidia.com/gpu=all &lt;IMAGE&gt; &lt;COMMAND&gt;\n</code></pre> Error: setting up CDI devices <p>The folloiwng error</p> <pre><code>Error: setting up CDI devices: failed to inject devices: failed to stat CDI host device \"/dev/nvidia-uvm\": no such file or directory\n</code></pre> <p>can be fixed by generating the CDI specification with <code>nvidia-ctk</code>, as shown above.</p>"},{"location":"containers/cscs/","title":"Containers at CSCS","text":""},{"location":"containers/cscs/#jfrog","title":"JFrog","text":""},{"location":"containers/cscs/#pull-a-container-from-jfrog-locally","title":"Pull a container from JFrog locally","text":"<ol> <li>Enable the VPN (if not on the CSCS network)</li> <li>Log in to JFrog    <pre><code>podman login --username &lt;USERNAME&gt; --password &lt;PASSWORD&gt; jfrog.svc.cscs.ch\n</code></pre>    The password is the JFrog API key, which can be generated in the JFrog web interface:    <code>User Menu &gt; Edit Profile &gt; API Key</code></li> <li>Pull the container    <pre><code>podman image pull jfrog.svc.cscs.ch/&lt;REPOSITORY&gt;/&lt;IMAGE&gt;:&lt;TAG&gt;\n</code></pre></li> </ol>"},{"location":"containers/cscs/#mpi-containers","title":"MPI Containers","text":""},{"location":"containers/cscs/#mpich","title":"MPICH","text":"<p>The following containers were created by Andreas Fink.</p>"},{"location":"containers/cscs/#eiger-zen2","title":"Eiger (zen2)","text":"<pre><code>FROM docker.io/ubuntu:24.04\n\nARG libfabric_version=1.22.0\nARG mpi_version=4.3.1\nARG osu_version=7.5.1\n\nRUN apt-get update \\\n    &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends build-essential ca-certificates automake autoconf libtool make gdb strace wget python3 git gfortran \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN git clone https://github.com/hpc/xpmem \\\n    &amp;&amp; cd xpmem/lib \\\n    &amp;&amp; gcc -I../include -shared -o libxpmem.so.1 libxpmem.c \\\n    &amp;&amp; ln -s libxpmem.so.1 libxpmem.so \\\n    &amp;&amp; mv libxpmem.so* /usr/lib64 \\\n    &amp;&amp; cp ../include/xpmem.h /usr/include/ \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd ../../ \\\n    &amp;&amp; rm -Rf xpmem\n\nRUN wget -q https://github.com/ofiwg/libfabric/archive/v${libfabric_version}.tar.gz \\\n    &amp;&amp; tar xf v${libfabric_version}.tar.gz \\\n    &amp;&amp; cd libfabric-${libfabric_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf v${libfabric_version}.tar.gz libfabric-${libfabric_version}\n\nRUN wget -q https://www.mpich.org/static/downloads/${mpi_version}/mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; tar xf mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; cd mpich-${mpi_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr --enable-fast=O3,ndebug --enable-fortran --enable-cxx --with-device=ch4:ofi --with-libfabric=/usr --with-xpmem=/usr \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf mpich-${mpi_version}.tar.gz mpich-${mpi_version}\n\nRUN wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; tar xf osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; cd osu-micro-benchmarks-v${osu_version} \\\n    &amp;&amp; ./configure --prefix=/usr/local CC=$(which mpicc) CFLAGS=-O3 \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf osu-micro-benchmarks-v${osu_version} osu-micro-benchmarks-v${osu_version}.tar.gz\n</code></pre> <p>The container engine will inject <code>xpmem</code> and <code>libfabric</code> into the container at runtime.</p> Building and running the container <p>Build the container:</p> <pre><code>podman build -f Dockerfile -t osu:latest .\nenroot import -o osu.sqfs podman://osu:latest # (1)!\n</code></pre> <ol> <li>Import the container with enroot for running with the container engine.</li> </ol> <p>Prepare the TOML file for the container engine:</p> <pre><code>image = '&lt;IMAGE&gt;'\nmounts = [\"/capstor/scratch/cscs/\"]\nworkdir = '&lt;WORKDIR&gt;'\nwritable = true\nentrypoint = true\nannotations.com.hooks.cxi.enabled = 'true' # (1)!\n</code></pre> <ol> <li>CXI hook.</li> </ol> <p>Run the [OSU micro-benchmarks]:</p> <pre><code>srun --mpi=pmi2 -p debug -N2 -n2 --environment=$PWD/osu.toml \\\n  /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw\n</code></pre>"},{"location":"containers/cscs/#daint-gh200","title":"Daint (GH200)","text":"<pre><code>FROM docker.io/nvidia/cuda:12.8.1-devel-ubuntu24.04 # (1)!\n\nARG libfabric_version=1.22.0\nARG mpi_version=4.3.1\nARG osu_version=7.5.1\n\nRUN apt-get update \\\n    &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends build-essential ca-certificates automake autoconf libtool make gdb strace wget python3 git gfortran \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN echo '/usr/local/cuda/lib64/stubs' &gt; /etc/ld.so.conf.d/cuda_stubs.conf &amp;&amp; ldconfig # (2)!\n\nRUN git clone https://github.com/hpc/xpmem \\\n    &amp;&amp; cd xpmem/lib \\\n    &amp;&amp; gcc -I../include -shared -o libxpmem.so.1 libxpmem.c \\\n    &amp;&amp; ln -s libxpmem.so.1 libxpmem.so \\\n    &amp;&amp; mv libxpmem.so* /usr/lib \\\n    &amp;&amp; cp ../include/xpmem.h /usr/include/ \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd ../../ \\\n    &amp;&amp; rm -Rf xpmem\n\nRUN wget -q https://github.com/ofiwg/libfabric/archive/v${libfabric_version}.tar.gz \\\n    &amp;&amp; tar xf v${libfabric_version}.tar.gz \\\n    &amp;&amp; cd libfabric-${libfabric_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr --with-cuda=/usr/local/cuda \\ # (3)!\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf v${libfabric_version}.tar.gz libfabric-${libfabric_version}\n\nRUN wget -q https://www.mpich.org/static/downloads/${mpi_version}/mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; tar xf mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; cd mpich-${mpi_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr --enable-fast=O3,ndebug --enable-fortran --enable-cxx --with-device=ch4:ofi --with-libfabric=/usr --with-xpmem=/usr --with-cuda=/usr/local/cuda \\ # (4)!\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf mpich-${mpi_version}.tar.gz mpich-${mpi_version}\n\nRUN wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; tar xf osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; cd osu-micro-benchmarks-v${osu_version} \\\n    &amp;&amp; ./configure --prefix=/usr/local --with-cuda=/usr/local/cuda CC=$(which mpicc) CFLAGS=-O3 \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf osu-micro-benchmarks-v${osu_version} osu-micro-benchmarks-v${osu_version}.tar.gz\n\nRUN rm /etc/ld.so.conf.d/cuda_stubs.conf &amp;&amp; ldconfig\n</code></pre> <ol> <li>Use the NVIDIA CUDA container as base image.</li> <li>Add <code>/usr/local/cuda/lib64/stubs</code> as default linking directory during the build process.    This is required because at build time no CUDA driver/GPU is available.    This path is removed at the end of the build process.</li> <li>Build <code>libfabric</code> with CUDA support.</li> <li>Build <code>MPICH</code> with CUDA support.</li> </ol> Building and running the container <p>Build the container:</p> <pre><code>podman build -f Dockerfile -t osu:latest .\nenroot import -o osu.sqfs podman://osu:latest # (1)!\n</code></pre> <ol> <li>Import the container with enroot for running with the container engine.</li> </ol> <p>Prepare the TOML file for the container engine:</p> <pre><code>image = '&lt;IMAGE&gt;'\nmounts = [\"/capstor/scratch/cscs/\"]\nworkdir = '&lt;WORKDIR&gt;'\nwritable = true\nentrypoint = true\nannotations.com.hooks.cxi.enabled = 'true' # (1)!\n</code></pre> <ol> <li>CXI hook.</li> </ol> <p>Run the [OSU micro-benchmarks] device-to-device (<code>D D</code>) bandwidth test:</p> <pre><code>srun --mpi=pmi2 -p debug -N2 -n2 --environment=$PWD/osu.toml \\\n  env MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1 \\ # (1)!\n  /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw D D\n</code></pre> <ol> <li>Enables GPU direct RDMA support in the provider.    Equivalent to Cray-MPICH <code>MPICH_GPU_SUPPORT_ENABLED=1</code>.</li> </ol> <p>Run the [OSU micro-benchmarks] host-to-host (<code>H H</code>) bandwidth test:</p> <pre><code>srun --mpi=pmi2 -p debug -N2 -n2 --environment=$PWD/osu.toml \\\n  /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw H H\n</code></pre>"},{"location":"develop/cmake/","title":"CMake","text":""},{"location":"develop/cmake/#cmake-and-language-servers","title":"CMake and language servers","text":"<p>CMake works well with the clangd language server. It can automatially generate the <code>compile_commands.json</code> file, which is used by clangd to provide code completion, diagnostics, and other features.</p> <p>The <code>compile_commands.json</code> contains the exact comiler call for all translation units of the project.</p> <p>To generate the <code>compile_commands.json</code> file, use the following CMake option:</p> <pre><code>-DCMAKE_EXPORT_COMPILE_COMMANDS=ON\n</code></pre>"},{"location":"develop/cmake/#ccache","title":"Ccache","text":"<p>To use Ccache (compiler cache) with CMake, one can set the following environment or CMake variable:</p> <pre><code>CMAKE_&lt;LANG&gt;_COMPILER_LAUNCHER=ccache\n</code></pre> <p>Supported languages are the following: <code>C</code>, <code>CXX</code>, <code>Fortran</code>, <code>CUDA</code>, <code>HIP</code>.</p>"},{"location":"develop/cmake/#project-wide-setting","title":"Project-wide setting","text":"<p>For a project-wide setting, one can add the following to the <code>CMakeLists.txt</code> file:</p> <pre><code>find_program(CCACHE_PROGRAM ccache)\nif(CCACHE_PROGRAM)\n  set(CMAKE_&lt;LANG&gt;_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\")\nendif()\n</code></pre>"},{"location":"develop/cmake/#target-specific-setting","title":"Target-specific setting","text":"<p>For a specific target, the <code>&lt;LANG&gt;_COMPILER_LAUNCHER</code> property can be set:</p> <pre><code>find_program(CCACHE_PROGRAM ccache)\nif(CCACHE_PROGRAM)\n  set_target_properties(&lt;TARGET&gt; PROPERTIES &lt;LANG&gt;_COMPILER_LAUNCHER ${CCACHE_PROGRAM})\nendif()\n</code></pre>"},{"location":"develop/spack/","title":"Spack","text":""},{"location":"develop/spack/#testing-locally-with-spack","title":"Testing locally with Spack","text":"<p>The Spack package manager is written in Python, therefore it can be used interactively from a Python shell:</p> <pre><code>&gt; spack python\n</code></pre>"},{"location":"develop/spack/#concretize-and-inspect-a-spec","title":"Concretize and inspect a spec","text":"<p>A Spack spec can be concretized and inspected from the Python shell:</p> <pre><code>from spack.spec import Spec\nfrom spack.concretize import concretize_one\n\ns = Spec(\"spfft ^[virtuals=fftw-api] nvpl-fft\") # (1)!\nsc = concretize_one(s) # (2)!\n</code></pre> <ol> <li>Define the Spack spec to concretize.</li> <li>Concretize the spec.</li> </ol> <p>One can then inspect the concretized spec, for example:</p> <pre><code>sc[\"fftw-api\"].libs.ld_flags\nsc[\"fftw-api\"].headers\n</code></pre>"},{"location":"develop/spack/#conflicts","title":"Conflicts","text":"<p>The following conflicts are equivalent:</p> <pre><code>conflicts(\"+rocm +cuda\")\nconflicts(\"+rocm\", when=\"+cuda\")\nconflicts(\"+cuda\", when=\"+rocm\")\n</code></pre> <p>The first option is cleaner, shorter, and less asymmetric.</p>"},{"location":"develop/spack/#ccache","title":"Ccache","text":"<p>To use Ccache (compiler cache) with Spack, one can set the following in <code>~/.spack/config.yaml</code> (or Spack's default <code>etc/spack/config.yaml</code>):</p> <pre><code>config:\n    ccache: true\n</code></pre>"},{"location":"develop/spack/#spack-views-for-development-tools","title":"Spack views for development tools","text":"<p>Spack environment views are a way to create a single directory that contains all the dependencies of a package. This is useful for easily installing and accessing development tools (such as editors, languages, etc.).</p> <pre><code>spack:\n  specs:\n    - &lt;SPEC1&gt;\n    - &lt;SPEC2&gt;\n  concretizer:\n    unify: true\n  view:\n    default:\n      root: &lt;VIEW_PATH&gt;\n</code></pre> Development tools <pre><code>spack:\n  specs:\n    - libtree\n    - tmux\n    - direnv\n    - htop +hwloc\n    - neovim\n    - llvm\n    - ripgrep\n    - fzf\n    - bat\n    - ruby\n    - go\n    - rust +dev ~docs\n    - node-js\n    - npm\n    - ccache\n    - python\n    - py-uv\n  concretizer:\n    unify: true\n  packages:\n    python:\n      require:\n        - \"@3.11\"\n    llvm:\n      require: \n        - \"targets=x86\" # Select one target to speedup compilation, doen't matter which one\n        - \"@18\"\n        - \"~gold\"\n        - \"~libomptarget\"\n  view:\n    default:\n  root: ~/aarch64-dev.view\n      link: roots\n</code></pre> <p>Views and the number of symlinks</p> <p>Views can create a large number of symlinks, which can cause issues with some file systems.</p> Count the number of symlinks <pre><code>find &lt;PATH&gt; -type l | wc -l\n</code></pre> <p>To avoid creating too many symlinks, one can use <code>link: roots</code>:</p> <pre><code>view:\n  default:\n    root: ~/aarch64-dev.view\n    link: roots\n</code></pre>"},{"location":"develop/spack/#use-spack-to-install-dependencies-and-run-cmake","title":"Use Spack to install dependencies and run CMake","text":"<p>The following script can be used to install dependencies and run CMake for a given develop spec:</p> <pre><code>#!/bin/bash\n\n# The script needs to be sourced to work correctly\n# Use return instead of exit to return to the shell\n\nhelp() {\n    echo \"Usage: source spackdev.sh &lt;spack-env&gt; &lt;spack-spec&gt;\"\n    echo \"  spack-env: Spack environment (path)\"\n    echo \"  spack-spec: Spack spec\"\n}\n\nerror_dev(){\n    echo \"ERROR\u250c ${1} is not a 'develop' spec.\"\n    echo \"ERROR\u2514  Make sure you are using the correct Spack environment and spec.\"\n    return 1\n}\n\n# Warn if build_stage is not set in the Spack environment\nwarn_bs() {\n    echo \"WARN\u250c You are using a standard 'build_stage' directory.\"\n    echo \"WARN| Consider adding the following to your Spack environment:\"\n    echo \"WARN|\"\n    echo \"WARN|     config:\"\n    echo \"WARN\u2514   build_stage: &lt;path-to-git-repo&gt;/spack-build-stage/\"\n}\n\nif [[ $# -ne 2 ]]; then\n    help\n    return\nfi\n\nSPACK_ENV=$1\nSPACK_SPEC=$2\n\nif command -v ccache 2&gt;&amp;1 &gt; /dev/null; then\n   export CMAKE_CXX_COMPILER_LAUNCHER=ccache\n   export CMAKE_C_COMPILER_LAUNCHER=ccache\n   export CMAKE_Fortran_COMPILER_LAUNCHER=ccache\n   export CMAKE_CUDA_COMPILER_LAUNCHER=ccache\n   export CMAKE_HIP_COMPILER_LAUNCHER=ccache\nfi\n\n# WARN: This only work if $SPACK_SPEC is just after the 'develop' key\ngrep -A 2 \"develop:\" \"${SPACK_ENV}/spack.yaml\" | grep -q \"${SPACK_SPEC}:\" || error_dev \"${SPACK_SPEC}\" || return\ngrep -q \"build_stage:\" \"${SPACK_ENV}/spack.yaml\" || warn_bs\n\nspack -e \"${SPACK_ENV}\" clean || return # (1)!\nspack -e \"${SPACK_ENV}\" concretize -f || return # (2)!\nspack -e \"${SPACK_ENV}\" install --until=cmake --test=root --keep-stage || return # (3)!\n\nSPACK_SOURCE_DIR=$(spack -e \"${SPACK_ENV}\" location --source-dir \"${SPACK_SPEC}\")\nSPACK_BUILD_DIR=$(spack -e \"${SPACK_ENV}\" location --build-dir \"${SPACK_SPEC}\")\n\nSPACK_ENV_NAME=$(basename \"${SPACK_ENV}\")\nENVRC_TMP=\"/tmp/.envrc-${SPACK_ENV_NAME}-${SPACK_SPEC}\"\nspack -e \"${SPACK_ENV}\" build-env --dump \"${ENVRC_TMP}\" \"${SPACK_SPEC}\" || return # (4)!\necho \"SPACK_BUILD_DIR=\\\"${SPACK_BUILD_DIR}\\\"; export SPACK_BUILD_DIR\" &gt;&gt; \"${ENVRC_TMP}\"\necho \"SPACK_SOURCE_DIR=\\\"${SPACK_SOURCE_DIR}\\\"; export SPACK_SOURCE_DIR\" &gt;&gt; \"${ENVRC_TMP}\"\n\ncp \"${ENVRC_TMP}\" \"${SPACK_BUILD_DIR}/.envrc\"\ndirenv allow \"${SPACK_BUILD_DIR}\"\n\n# Add .envrc in SPACK_SOURCE_DIR so that nvim integrated terminal is correctly set up\n mv \"${ENVRC_TMP}\" \"${SPACK_SOURCE_DIR}/.envrc\"\n direnv allow \"${SPACK_SOURCE_DIR}\"\n\n# Be friendly to LSPs\nmkdir -p \"${SPACK_SOURCE_DIR}/build\"\nln -sf \"${SPACK_BUILD_DIR}/compile_commands.json\" \"${SPACK_SOURCE_DIR}/build/compile_commands.json\" # (5)!\n\n# Remove broken symlinks (from previous builds)\nfind \"${SPACK_SOURCE_DIR}\" -xtype l -delete\n\npushd \"${SPACK_SOURCE_DIR}\" || return\n</code></pre> <ol> <li>Clean the Spack environment.</li> <li>Concretize the environment.</li> <li>Install the dependencies and run CMake.</li> <li>Dump the build environment to an <code>.envrc</code> file. This is used by direnv to set up the environment.</li> <li>Create a symlink to the <code>compile_commands.json</code> file in the source directory. This is useful for LSPs.</li> </ol> <p>Note</p> <p>The script is meant to be sourced (<code>source ...</code>), not executed.</p>"},{"location":"editors/vscode/","title":"VScode","text":""},{"location":"editors/vscode/#open-window-from-integrated-terminal","title":"Open Window from Integrated Terminal","text":"<p>Open a file in the current window from the integrated terminal: <pre><code>code -r &lt;FILE&gt;\n</code></pre> or <pre><code>code --reuse-window &lt;FILE&gt;\n</code></pre></p>"},{"location":"git/git/","title":"<code>git</code>","text":""},{"location":"git/git/#worktrees","title":"Worktrees","text":"<p><code>git worktree</code> allows to have multiple working directories for a single repository. A worktree is the same as a normal repository, with the caviet that it is not possible to checkeout the same branch in multiple worktrees.</p> <p>This is useful for working on multiple features in parallel, and avoids the need to stash changes or create multiple clones of the repository.</p>"},{"location":"git/git/#create-a-new-worktree","title":"Create a new worktree","text":"<pre><code>git worktree add -b &lt;BRANCH&gt; &lt;PATH&gt;\n</code></pre> <p>Convention</p> <p>Clone the repository as <code>&lt;REPO&gt;.&lt;DEFAULT_BRANCH&gt;</code> and create worktrees for features as <code>&lt;REPO&gt;.&lt;FEATURE_BRANCH&gt;</code>. This is my own convention and it is not a standard.</p>"},{"location":"git/git/#aliases","title":"Aliases","text":"<p><code>git</code> allows to create aliases for common commands.</p> <p>Aliases can be created in the global configuration as follows:</p> <pre><code>git config --global alias.&lt;ALIAS&gt; &lt;COMMAND&gt;\n</code></pre> <p>My configuration</p> <pre><code>[alias]\n    aa = add --all\n    co = checkout\n    cob = checkout -b\n    cs = commit -signed --signoff\n    discard = reset HEAD --hard\n    fp = push --force-with-lease\n    lol = log --graph --decorate --pretty=oneline --abbrev-commit\n    lola = log --graph --decorate --pretty=oneline --abbrev-commit --all\n    pr = \"!f() { git fetch --force origin pull/$1/head:pr-$1; }; f\"\n    ps = push --signed\n    root = rev-parse --show-toplevel\n    st = status\n    stag = tag -l --sort=v:refname\n    uncommit = reset --soft HEAD^\n    unstage = reset HEAD --\n    wt = worktree\n</code></pre> <code>--force-with-lease</code> <p><code>--force-with-lease</code> is a safer alternative to <code>--force</code> that does not overwrite changes on the remote branch.</p>"},{"location":"git/git/#identity","title":"Identity","text":""},{"location":"git/git/#signing-commits","title":"Signing commits","text":"<p><code>git</code> supports signing commits with GPG keys.</p> Create GPG keys <pre><code>gpg --default-new-key-algo rsa4096 --gen-key\ngpg --armor --export &lt;KEY_ID&gt; # (1)!\n</code></pre> <ol> <li>Export public key. This can be used to add the key to GitHub, etc.</li> </ol> List available GPG keys <pre><code>gpg --list-secret-keys --keyid-format=long\n</code></pre> <pre><code>git config --global --unset gpg.format # (1)!\ngit config --global user.signingkey 3AA5C34371567BD2\n</code></pre> <ol> <li>One could use different key formats, such as SSH keys. Here we want to use GPG keys.</li> </ol> <p>Signing keys can also be added to specific configurations or repositories. See Conditional configuration for separate work and personal configurations.</p>"},{"location":"git/git/#signoff-commits","title":"Signoff commits","text":"<p><code>git commit</code> supports the <code>--signoff</code> option to add a signoff line to the commit message.</p> <p>This is useful to indicate that the author has the right to submit the code.</p> <pre><code>git commit --signoff\n</code></pre>"},{"location":"git/git/#bisection","title":"Bisection","text":"<p><code>git bisect</code> allows to find the commit that introduced a bug by performing a binary search.</p> <p>To starti bisecting, use:</p> <pre><code>git bisect start\n</code></pre> <p>Then mark the current commit as bad:</p> <p><pre><code>git bisect bad\n</code></pre> and mark a known good commit:</p> <pre><code>git bisect good &lt;COMMIT&gt;\n</code></pre> <p><code>git bisect</code> will then checkout a commit in the middle of the range and ask to mark it as good or bad. This process is repeated until the commit that introduced the bug is found.</p>"},{"location":"git/git/#correct-a-bisection-mistake","title":"Correct a bisection mistake","text":"<p>If a commit is marked as good or bad by mistake, it can be corrected with the following steps:</p> <ol> <li> <p>Save the current bisection log to a file    <pre><code>git bisect log &gt; bisect.log\n</code></pre></p> </li> <li> <p>Reset the bisection state    <pre><code>git bisect reset\n</code></pre></p> </li> <li> <p>Amend the bisection log file to remove the incorrect entry</p> </li> <li> <p>Restart the bisection with the corrected log    <pre><code>git bisect replay bisect.log\n</code></pre></p> </li> </ol> <code>git bisect</code> log <pre><code>git bisect start\n# status: waiting for both good and bad commits\n# good: [0128dc24c6d018b61ceaac080640014e1d5ec344] Update README\ngit bisect good 0128dc24c6d018b61ceaac080640014e1d5ec344\n# bad: [25935e1a7e022ede9fd71bd86dcbaa7a3f1846b7] Merge pull request #85 from gdonval/fix-malloc\ngit bisect bad 25935e1a7e022ede9fd71bd86dcbaa7a3f1846b7\n# bad: [637a6e5f2953263d4f05574c8d6037356a81b9ea] Merge pull request #65 from Reference-ScaLAPACK/weslleyspereira-fix-cmake-requirements\ngit bisect bad 637a6e5f2953263d4f05574c8d6037356a81b9ea\n# good: [681a6cf436360a3f5121f224a026150795283e56] Merge pull request #56 from weslleyspereira/try-CI\ngit bisect good 681a6cf436360a3f5121f224a026150795283e56\n</code></pre> <p>Tip</p> <p>One could add the following git alias to undo the last action automatically:</p> <pre><code>alias bisect-undo='git bisect log | head -n -2 &gt; /tmp/git-fixed-bisect.txt &amp;&amp; git bisect reset &amp;&amp; git bisect replay /tmp/git-fixed-bisect.txt &amp;&amp; rm /tmp/git-fixed-bisect.txt'\n</code></pre>"},{"location":"git/git/#cherrypicking-changes","title":"Cherrypicking changes","text":"<p><code>git add</code> allows to interactively select hunks to stage form different files:</p> <pre><code>git add -p\n</code></pre> <p>This allows to create atomic commits by interactively selecting changes to be staged.</p>"},{"location":"git/git/#large-repositories","title":"Large Repositories","text":"<p>When cloning large repositories, setting <code>feature.manyFiles</code> enables configuration options to improve performance:</p> <pre><code>git clone -c feature.manyFiles=true &lt;REPO&gt;\n</code></pre>"},{"location":"git/git/#diff-for-staged-files","title":"Diff for staged files","text":"<p><code>git diff</code> does not show staged files in the diff.</p> <p>The <code>--staged</code> option can be used to show the diff for staged files:</p> <pre><code>git diff --staged\n</code></pre>"},{"location":"git/git/#global-ignore-file","title":"Global ignore file","text":"<p>A global <code>git</code> ignore file can be created to ignore files across all repositories. This is useful for files that are specific to a certain workflow (<code>.envrc</code>, ...) or editor (<code>.vscode</code>, <code>.idea</code>),  but not relevant to the repository.</p> <p>The global ignore file can be created by setting the <code>core.excludesfile</code> configuration:</p> <pre><code>git config --global core.excludesfile ~/.config/git/ignore\n</code></pre> <p>Typically, the file is stored under <code>~/.config/git/ignore</code>. To check if the global ignore file already exists, use:</p> <pre><code>git config --get core.excludesfile\n</code></pre> Excluded files <pre><code>.envrc\n.vscode/\n.idea/\ncore*\n.null-ls_*\n</code></pre>"},{"location":"git/git/#conditional-configuration","title":"Conditional configuration","text":"<p><code>git</code> supports conditional configurations to include configurations based on conditions. A common use-case is to have different configurations for work and personal repositories.</p> <p>Separate work and personal configurations</p> <p>If work repositories are under <code>~/git/work</code> and personal repositories are under <code>~/git/oss</code>, the following configuration can be used:</p> <pre><code>[includeIf \"gitdir:~/git/work/\"]\n    path = ~/git/work/.config\n[includeIf \"gitdir:~/git/oss/\"]\n    path = ~/git/oss/.config\n</code></pre> <code>~/git/work/.config</code><code>~/git/oss/.config</code> <pre><code>[user]\n    name = John Doe\n    email = john.doe@work.com\n</code></pre> <pre><code>[user]\n    name = John Doe\n    email = john.doe@oss.org\n</code></pre>"},{"location":"git/git/#clean-up","title":"Clean up","text":"<p>Some workflows produce a lot of temporary files that are not tracked by <code>git</code>. It is sometimes useful to clean up these files to have the repository in a clean state. To clean up untracked files, use:</p> <pre><code>git clean -fd\n</code></pre> <p>The command above will remove all untracked files and directories.</p> <p>Danger</p> <p>Be careful with <code>git clean -fd</code> as it will remove all untracked files and directories. It is recommended to perform a dry run with</p> <pre><code>git clean -nfd\n</code></pre> <p>to see what files will be removed.</p>"},{"location":"git/git/#revert-changes-from-a-specific-file","title":"Revert changes from a specific file","text":"<p>The <code>git revert</code> command can be used to revert a specific commit. However, this reverts all the files changed in that commit. To revert changes from a specific file, use the <code>git checkout</code> command:</p> <pre><code>git checkout &lt;COMMIT&gt;~ -- &lt;FILE&gt;\n</code></pre> <p><code>~</code></p> <p><code>&lt;rev&gt;~[&lt;n&gt;]</code> refers to the <code>&lt;n&gt;</code>-th generation ancestor of <code>&lt;rev&gt;</code>.</p> <p>To revert the changes introduced by a specific commit <code>&lt;COMMIT&gt;</code>, we checkout the file from the commit before it (<code>&lt;COMMIT&gt;~</code>).</p>"},{"location":"git/github/","title":"GitHub","text":""},{"location":"git/github/#list-files-in-active-prs","title":"List files in active PRs","text":"<p>To apply repository-wide automated changes (such as formatting) in active repositories, it is useful to list files in active PRs to minimize the impact of such changes.</p> <p>The GitHub CLI can be used to obtain such information. The following extracts information, in JSON format, about 100 open PRs:</p> <pre><code>gh pr list -L 100 --json number,updatedAt,createdAt,title\n</code></pre> <p>jq can then be used to interact with the JSON output.</p> <p>To extract the list of PR numbers with updates after a certain date one can use the folloiwng query:</p> <pre><code>gh pr list -L 500 --json number,updatedAt,createdAt,title \\\n  | jq '[.[] | select(.updatedAt &gt; (\"2025-07-01\"))]' \\\n  | jq '.[] | .number'\n</code></pre> <p>With the PR numbers at hand, one can list the modified files in each PR using [gh] again:</p> <pre><code>gh pr diff --name-only &lt;PR_NUMBER&gt;\n</code></pre> Credits <p>This workflow was originally developed by Egor Marin, who helped with the automatic formatting of the whole MDAnalysis code base.</p>"},{"location":"git/github/#secret-files","title":"Secret files","text":"<p>GitHub Actions might require a secret file.  Such file could be stored encrypted in the GitHub repository and the decryption passphrase can be stored as a GitHub Secret.</p>"},{"location":"git/github/#encrypting-secret-file-with-gpg","title":"Encrypting Secret File with GPG","text":"<p>Encrypt file with GPG:</p> <pre><code>gpg --symmetric --cipher-algo AES256 FILE\n</code></pre> <p>The passphrase used at this step is required to decrypt the file and can be stored as a GitHub Secret.</p>"},{"location":"git/github/#decryption-secret-file","title":"Decryption Secret File","text":"<pre><code>#!/bin/sh\n\nmkdir $HOME/secrets\n\ngpg --quiet --batch --yes --decrypt --passphrase=\"$SECRET_PASSPHRASE\" \\\n    --output $HOME/secrets/FILE FILE.gpg\n</code></pre> <p>The shell script has to  be executable when pushed to GitHub:</p> <pre><code>chmod +x decrypt_secret.sh\n</code></pre>"},{"location":"git/github/#github-action","title":"GitHub Action","text":"<pre><code>name: Decrypt Secret File\n\non: push\n\njobs:\n  my-job:\n    name: Decrypt Secret File\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Decrypt Secret File\n        run: ./.github/scripts/decrypt_secret.sh\n        env:\n          SECRET_PASSPHRASE: ${{ secrets.SECRET_PASSPHRASE }}\n</code></pre>"},{"location":"hpc/affinity/","title":"CPU affinity","text":""},{"location":"hpc/affinity/#list-running-threads-and-their-cpu","title":"List running threads and their CPU","text":"<p>To see which core each thread of a given application is running on, one can use the following command:</p> <pre><code>for i in $(pgrep APPLICATION); do ps -mo pid,tid,fname,user,psr -p $i; done\n</code></pre> <p>PSR is the processor that process is currently assigned to.</p>"},{"location":"hpc/affinity/#print-affinity-masks","title":"Print affinity masks","text":"<p>An affinity mask is a bitmask where indices correspond to logical processors. The least significant bit corresponds to the first logical processor number on the system, while the most significant bit corresponds to the last logical processor number on the system.</p>"},{"location":"hpc/affinity/#hwloc-bind","title":"<code>hwloc-bind</code>","text":"<pre><code>srun [...] bash -c 'printf \"%02g:%s\\n\" ${SLURM_LOCALID} $(hwloc-bind --get --taskset)' | sort\n</code></pre> Output <pre><code>$ srun --cpus-per-task 8 -N 1 --ntasks 32 bash -c 'printf \"%02g:%s\\n\" ${SLURM_LOCALID} $(hwloc-bind --get --taskset)' | sort\n00:0x1fe\n01:0x1fe000000000000000000\n02:0x1fe000000000000000000000000000000000000\n03:0x1fe000000000000000000000000000000000000000000000000000000\n04:0x1fe00\n05:0x1fe00000000000000000000\n06:0x1fe00000000000000000000000000000000000000\n07:0x1fe00000000000000000000000000000000000000000000000000000000\n08:0x1fe0000\n09:0x1fe0000000000000000000000\n10:0x1fe0000000000000000000000000000000000000000\n11:0x1fe0000000000000000000000000000000000000000000000000000000000\n12:0x1fe000000\n13:0x1fe000000000000000000000000\n14:0x1fe000000000000000000000000000000000000000000\n15:0x1fe000000000000000000000000000000000000000000000000000000000000\n16:0x1fe00000000\n17:0x1fe00000000000000000000000000\n18:0x1fe00000000000000000000000000000000000000000000\n19:0x1fe00000000000000000000000000000000000000000000000000000000000000\n20:0x1fe0000000000\n21:0x1fe0000000000000000000000000000\n22:0x1fe0000000000000000000000000000000000000000000000\n23:0x1fe0000000000000000000000000000000000000000000000000000000000000000\n24:0x1fe000000000000\n25:0x1fe000000000000000000000000000000\n26:0x1fe000000000000000000000000000000000000000000000000\n27:0x1fe000000000000000000000000000000000000000000000000000000000000000000\n28:0x1fe00000000000000\n29:0x1fe00000000000000000000000000000000\n30:0x1fe00000000000000000000000000000000000000000000000000\n31:0x1fe00000000000000000000000000000000000000000000000000000000000000000000\n</code></pre>"},{"location":"hpc/affinity/#slurm","title":"Slurm","text":"<pre><code>srun [...] --cpu-bind=verbose\n</code></pre> Output <pre><code>cpu-bind=MASK - nid005367, task  0  0 [43161]: mask 0x1fe set\ncpu-bind=MASK - nid005367, task  1  1 [43162]: mask 0x1fe000000000000000000 set\ncpu-bind=MASK - nid005367, task  2  2 [43163]: mask 0x1fe000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task  3  3 [43164]: mask 0x1fe000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task  4  4 [43165]: mask 0x1fe00 set\ncpu-bind=MASK - nid005367, task  5  5 [43166]: mask 0x1fe00000000000000000000 set\ncpu-bind=MASK - nid005367, task  6  6 [43167]: mask 0x1fe00000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task  7  7 [43168]: mask 0x1fe00000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task  8  8 [43169]: mask 0x1fe0000 set\ncpu-bind=MASK - nid005367, task  9  9 [43170]: mask 0x1fe0000000000000000000000 set\ncpu-bind=MASK - nid005367, task 10 10 [43171]: mask 0x1fe0000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 11 11 [43172]: mask 0x1fe0000000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 12 12 [43173]: mask 0x1fe000000 set\ncpu-bind=MASK - nid005367, task 13 13 [43174]: mask 0x1fe000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 14 14 [43175]: mask 0x1fe000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 15 15 [43176]: mask 0x1fe000000000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 16 16 [43177]: mask 0x1fe00000000 set\ncpu-bind=MASK - nid005367, task 17 17 [43178]: mask 0x1fe00000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 18 18 [43179]: mask 0x1fe00000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 19 19 [43180]: mask 0x1fe00000000000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 20 20 [43181]: mask 0x1fe0000000000 set\ncpu-bind=MASK - nid005367, task 21 21 [43182]: mask 0x1fe0000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 22 22 [43183]: mask 0x1fe0000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 23 23 [43184]: mask 0x1fe0000000000000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 24 24 [43185]: mask 0x1fe000000000000 set\ncpu-bind=MASK - nid005367, task 25 25 [43186]: mask 0x1fe000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 26 26 [43187]: mask 0x1fe000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 27 27 [43188]: mask 0x1fe000000000000000000000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 28 28 [43189]: mask 0x1fe00000000000000 set\ncpu-bind=MASK - nid005367, task 29 29 [43190]: mask 0x1fe00000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 30 30 [43191]: mask 0x1fe00000000000000000000000000000000000000000000000000 set\ncpu-bind=MASK - nid005367, task 31 31 [43192]: mask 0x1fe00000000000000000000000000000000000000000000000000000000000000000000 set\n</code></pre>"},{"location":"hpc/affinity/#openmp","title":"OpenMP","text":"<pre><code>export OMP_DISPLAY_AFFINITY=TRUE\n</code></pre> Output <pre><code>level 1 thread 0x40000a7e9900 affinity 249-256\nlevel 1 thread 0x400013adc900 affinity 249-256\nlevel 1 thread 0x400013cec900 affinity 249-256\nlevel 1 thread 0x400013efc900 affinity 249-256\nlevel 1 thread 0x40001410c900 affinity 249-256\nlevel 1 thread 0x40001431c900 affinity 249-256\nlevel 1 thread 0x40001452c900 affinity 249-256\nlevel 1 thread 0x4000426a9900 affinity 185-192\nlevel 1 thread 0x40004b99c900 affinity 185-192\nlevel 1 thread 0x40004bbac900 affinity 185-192\nlevel 1 thread 0x40004bdbc900 affinity 185-192\nlevel 1 thread 0x40004bfcc900 affinity 185-192\nlevel 1 thread 0x40004c1dc900 affinity 185-192\nlevel 1 thread 0x40004c3ec900 affinity 185-192\nlevel 1 thread 0x400020279900 affinity 265-272\nlevel 1 thread 0x40002956c900 affinity 265-272\nlevel 1 thread 0x40002977c900 affinity 265-272\nlevel 1 thread 0x40002998c900 affinity 265-272\nlevel 1 thread 0x400029b9c900 affinity 265-272\nlevel 1 thread 0x400029dac900 affinity 265-272\nlevel 1 thread 0x400029fbc900 affinity 265-272\nlevel 1 thread 0x400038c39900 affinity 225-232\nlevel 1 thread 0x400041f2c900 affinity 225-232\nlevel 1 thread 0x40004213c900 affinity 225-232\nlevel 1 thread 0x40004234c900 affinity 225-232\nlevel 1 thread 0x40004255c900 affinity 225-232\nlevel 1 thread 0x40004276c900 affinity 225-232\nlevel 1 thread 0x40004297c900 affinity 225-232\nlevel 1 thread 0x4000096a9900 affinity 233-240\nlevel 1 thread 0x40001299c900 affinity 233-240\nlevel 1 thread 0x400012bac900 affinity 233-240\nlevel 1 thread 0x400012dbc900 affinity 233-240\nlevel 1 thread 0x400012fcc900 affinity 233-240\nlevel 1 thread 0x4000131dc900 affinity 233-240\nlevel 1 thread 0x4000133ec900 affinity 233-240\nlevel 1 thread 0x40001da09900 affinity 129-136\nlevel 1 thread 0x400026cfc900 affinity 129-136\nlevel 1 thread 0x400026f0c900 affinity 129-136\nlevel 1 thread 0x40002711c900 affinity 129-136\nlevel 1 thread 0x40002732c900 affinity 129-136\nlevel 1 thread 0x40002753c900 affinity 129-136\nlevel 1 thread 0x40002774c900 affinity 129-136\nlevel 1 thread 0x400009619900 affinity 217-224\nlevel 1 thread 0x40001290c900 affinity 217-224\nlevel 1 thread 0x400012b1c900 affinity 217-224\nlevel 1 thread 0x400012d2c900 affinity 217-224\nlevel 1 thread 0x400012f3c900 affinity 217-224\nlevel 1 thread 0x40001314c900 affinity 217-224\nlevel 1 thread 0x40001335c900 affinity 217-224\nlevel 1 thread 0x400044149900 affinity 257-264\nlevel 1 thread 0x40004d43c900 affinity 257-264\nlevel 1 thread 0x40004d64c900 affinity 257-264\nlevel 1 thread 0x40004d85c900 affinity 257-264\nlevel 1 thread 0x40004da6c900 affinity 257-264\nlevel 1 thread 0x40004dc7c900 affinity 257-264\nlevel 1 thread 0x40004de8c900 affinity 257-264\nlevel 1 thread 0x4000406a9900 affinity 89-96\nlevel 1 thread 0x40004999c900 affinity 89-96\nlevel 1 thread 0x400049bac900 affinity 89-96\nlevel 1 thread 0x400049dbc900 affinity 89-96\nlevel 1 thread 0x400049fcc900 affinity 89-96\nlevel 1 thread 0x40004a1dc900 affinity 89-96\nlevel 1 thread 0x40004a3ec900 affinity 89-96\nlevel 1 thread 0x40000a7f9900 affinity 273-280\nlevel 1 thread 0x400013aec900 affinity 273-280\nlevel 1 thread 0x400013cfc900 affinity 273-280\nlevel 1 thread 0x400013f0c900 affinity 273-280\nlevel 1 thread 0x40001411c900 affinity 273-280\nlevel 1 thread 0x40001432c900 affinity 273-280\nlevel 1 thread 0x40001453c900 affinity 273-280\nlevel 1 thread 0x4000256a9900 affinity 49-56\nlevel 1 thread 0x40002e99c900 affinity 49-56\nlevel 1 thread 0x40002ebac900 affinity 49-56\nlevel 1 thread 0x40002edbc900 affinity 49-56\nlevel 1 thread 0x40002efcc900 affinity 49-56\nlevel 1 thread 0x40002f1dc900 affinity 49-56\nlevel 1 thread 0x40002f3ec900 affinity 49-56\nlevel 1 thread 0x4000301b9900 affinity 161-168\nlevel 1 thread 0x4000394ac900 affinity 161-168\nlevel 1 thread 0x4000396bc900 affinity 161-168\nlevel 1 thread 0x4000398cc900 affinity 161-168\nlevel 1 thread 0x400039adc900 affinity 161-168\nlevel 1 thread 0x400039cec900 affinity 161-168\nlevel 1 thread 0x400039efc900 affinity 161-168\nlevel 1 thread 0x4000137c9900 affinity 33-40\nlevel 1 thread 0x40001cabc900 affinity 33-40\nlevel 1 thread 0x40001cccc900 affinity 33-40\nlevel 1 thread 0x40001cedc900 affinity 33-40\nlevel 1 thread 0x40001d0ec900 affinity 33-40\nlevel 1 thread 0x40001d2fc900 affinity 33-40\nlevel 1 thread 0x40001d50c900 affinity 33-40\nlevel 1 thread 0x40003d509900 affinity 121-128\nlevel 1 thread 0x4000467fc900 affinity 121-128\nlevel 1 thread 0x400046a0c900 affinity 121-128\nlevel 1 thread 0x400046c1c900 affinity 121-128\nlevel 1 thread 0x400046e2c900 affinity 121-128\nlevel 1 thread 0x40004703c900 affinity 121-128\nlevel 1 thread 0x40004724c900 affinity 121-128\nlevel 1 thread 0x400037349900 affinity 9-16\nlevel 1 thread 0x40004063c900 affinity 9-16\nlevel 1 thread 0x40004084c900 affinity 9-16\nlevel 1 thread 0x400040a5c900 affinity 9-16\nlevel 1 thread 0x400040c6c900 affinity 9-16\nlevel 1 thread 0x400040e7c900 affinity 9-16\nlevel 1 thread 0x40004108c900 affinity 9-16\nlevel 1 thread 0x40001bdf9900 affinity 17-24\nlevel 1 thread 0x4000250ec900 affinity 17-24\nlevel 1 thread 0x4000252fc900 affinity 17-24\nlevel 1 thread 0x40002550c900 affinity 17-24\nlevel 1 thread 0x40002571c900 affinity 17-24\nlevel 1 thread 0x40002592c900 affinity 17-24\nlevel 1 thread 0x400025b3c900 affinity 17-24\nlevel 1 thread 0x4000409a9900 affinity 113-120\nlevel 1 thread 0x400049c9c900 affinity 113-120\nlevel 1 thread 0x400049eac900 affinity 113-120\nlevel 1 thread 0x40004a0bc900 affinity 113-120\nlevel 1 thread 0x40004a2cc900 affinity 113-120\nlevel 1 thread 0x40004a4dc900 affinity 113-120\nlevel 1 thread 0x40004a6ec900 affinity 113-120\nlevel 1 thread 0x40001d349900 affinity 73-80\nlevel 1 thread 0x40002663c900 affinity 73-80\nlevel 1 thread 0x40002684c900 affinity 73-80\nlevel 1 thread 0x400026a5c900 affinity 73-80\nlevel 1 thread 0x400026c6c900 affinity 73-80\nlevel 1 thread 0x400026e7c900 affinity 73-80\nlevel 1 thread 0x40002708c900 affinity 73-80\nlevel 1 thread 0x4000298a9900 affinity 145-152\nlevel 1 thread 0x400032b9c900 affinity 145-152\nlevel 1 thread 0x400032dac900 affinity 145-152\nlevel 1 thread 0x400032fbc900 affinity 145-152\nlevel 1 thread 0x4000331cc900 affinity 145-152\nlevel 1 thread 0x4000333dc900 affinity 145-152\nlevel 1 thread 0x4000335ec900 affinity 145-152\nlevel 1 thread 0x400005c79900 affinity 169-176\nlevel 1 thread 0x40000ef6c900 affinity 169-176\nlevel 1 thread 0x40000f17c900 affinity 169-176\nlevel 1 thread 0x40000f38c900 affinity 169-176\nlevel 1 thread 0x40000f59c900 affinity 169-176\nlevel 1 thread 0x40000f7ac900 affinity 169-176\nlevel 1 thread 0x40000f9bc900 affinity 169-176\nlevel 1 thread 0x40003a6b9900 affinity 105-112\nlevel 1 thread 0x4000439ac900 affinity 105-112\nlevel 1 thread 0x400043bbc900 affinity 105-112\nlevel 1 thread 0x400043dcc900 affinity 105-112\nlevel 1 thread 0x400043fdc900 affinity 105-112\nlevel 1 thread 0x4000441ec900 affinity 105-112\nlevel 1 thread 0x4000443fc900 affinity 105-112\nlevel 1 thread 0x400040ce9900 affinity 41-48\nlevel 1 thread 0x400049fdc900 affinity 41-48\nlevel 1 thread 0x40004a1ec900 affinity 41-48\nlevel 1 thread 0x40004a3fc900 affinity 41-48\nlevel 1 thread 0x40004a60c900 affinity 41-48\nlevel 1 thread 0x40004a81c900 affinity 41-48\nlevel 1 thread 0x40004aa2c900 affinity 41-48\nlevel 1 thread 0x40000bcc9900 affinity 193-200\nlevel 1 thread 0x400014fbc900 affinity 193-200\nlevel 1 thread 0x4000151cc900 affinity 193-200\nlevel 1 thread 0x4000153dc900 affinity 193-200\nlevel 1 thread 0x4000155ec900 affinity 193-200\nlevel 1 thread 0x4000157fc900 affinity 193-200\nlevel 1 thread 0x400015a0c900 affinity 193-200\nlevel 1 thread 0x40002fb39900 affinity 81-88\nlevel 1 thread 0x400038e2c900 affinity 81-88\nlevel 1 thread 0x40003903c900 affinity 81-88\nlevel 1 thread 0x40003924c900 affinity 81-88\nlevel 1 thread 0x40003945c900 affinity 81-88\nlevel 1 thread 0x40003966c900 affinity 81-88\nlevel 1 thread 0x40003987c900 affinity 81-88\nlevel 1 thread 0x40000e809900 affinity 153-160\nlevel 1 thread 0x400017afc900 affinity 153-160\nlevel 1 thread 0x400017d0c900 affinity 153-160\nlevel 1 thread 0x400017f1c900 affinity 153-160\nlevel 1 thread 0x40001812c900 affinity 153-160\nlevel 1 thread 0x40001833c900 affinity 153-160\nlevel 1 thread 0x40001854c900 affinity 153-160\nlevel 1 thread 0x400012829900 affinity 25-32\nlevel 1 thread 0x40001bb1c900 affinity 25-32\nlevel 1 thread 0x40001bd2c900 affinity 25-32\nlevel 1 thread 0x40001bf3c900 affinity 25-32\nlevel 1 thread 0x40001c14c900 affinity 25-32\nlevel 1 thread 0x40001c35c900 affinity 25-32\nlevel 1 thread 0x40001c56c900 affinity 25-32\nlevel 1 thread 0x400041a19900 affinity 97-104\nlevel 1 thread 0x40004ad0c900 affinity 97-104\nlevel 1 thread 0x40004af1c900 affinity 97-104\nlevel 1 thread 0x40004b12c900 affinity 97-104\nlevel 1 thread 0x40004b33c900 affinity 97-104\nlevel 1 thread 0x40004b54c900 affinity 97-104\nlevel 1 thread 0x40004b75c900 affinity 97-104\nlevel 1 thread 0x40003a3a9900 affinity 241-248\nlevel 1 thread 0x40004369c900 affinity 241-248\nlevel 1 thread 0x4000438ac900 affinity 241-248\nlevel 1 thread 0x400043abc900 affinity 241-248\nlevel 1 thread 0x400043ccc900 affinity 241-248\nlevel 1 thread 0x400043edc900 affinity 241-248\nlevel 1 thread 0x4000440ec900 affinity 241-248\nlevel 1 thread 0x400031339900 affinity 57-64\nlevel 1 thread 0x40003a62c900 affinity 57-64\nlevel 1 thread 0x40003a83c900 affinity 57-64\nlevel 1 thread 0x40003aa4c900 affinity 57-64\nlevel 1 thread 0x40003ac5c900 affinity 57-64\nlevel 1 thread 0x40003ae6c900 affinity 57-64\nlevel 1 thread 0x40003b07c900 affinity 57-64\nlevel 1 thread 0x40001a7d9900 affinity 201-208\nlevel 1 thread 0x400023acc900 affinity 201-208\nlevel 1 thread 0x400023cdc900 affinity 201-208\nlevel 1 thread 0x400023eec900 affinity 201-208\nlevel 1 thread 0x4000240fc900 affinity 201-208\nlevel 1 thread 0x40002430c900 affinity 201-208\nlevel 1 thread 0x40002451c900 affinity 201-208\nlevel 1 thread 0x40001a519900 affinity 177-184\nlevel 1 thread 0x40002380c900 affinity 177-184\nlevel 1 thread 0x400023a1c900 affinity 177-184\nlevel 1 thread 0x400023c2c900 affinity 177-184\nlevel 1 thread 0x400023e3c900 affinity 177-184\nlevel 1 thread 0x40002404c900 affinity 177-184\nlevel 1 thread 0x40002425c900 affinity 177-184\nlevel 1 thread 0x400038269900 affinity 1-8\nlevel 1 thread 0x40004155c900 affinity 1-8\nlevel 1 thread 0x40004176c900 affinity 1-8\nlevel 1 thread 0x40004197c900 affinity 1-8\nlevel 1 thread 0x400041b8c900 affinity 1-8\nlevel 1 thread 0x400041d9c900 affinity 1-8\nlevel 1 thread 0x400041fac900 affinity 1-8\n</code></pre> <p>The following could be a better setting for printing out the affinity:</p> <pre><code>export OMP_AFFINITY_FORMAT=\"host=%H, pid=%P, thread_num=%n, thread affinity=%A\"\n</code></pre>"},{"location":"hpc/affinity/#determine-current-cpu","title":"Determine current CPU","text":"<pre><code>#include &lt;sched.h&gt;\n\nint get_current_cpu() {\n    return sched_getcpu();\n}\n</code></pre>"},{"location":"hpc/affinity/#resources","title":"Resources","text":"<ul> <li>affinity, from Vasilis Karakasis (@vkarak)</li> </ul>"},{"location":"hpc/cluster/","title":"HPC Clusters","text":""},{"location":"hpc/cluster/#from-node-id-to-xname","title":"From node ID to xname","text":"<p>Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system.</p> <p>On the node, you can run the following command to get the xname:</p> <pre><code>cat /etc/cray/xname\n</code></pre> <p>The xname has the following format:</p> <pre><code>xXcCsSbBnN\n</code></pre> Field Description x Cabinet number c Chassis number s Slot number b Card number n Node number <p>For CSCS GH200 system, the node number is always <code>0</code> (<code>n0</code>). The card number can be either <code>0</code> or <code>1</code>, for the two nodes in the compute blade.</p>"},{"location":"hpc/mpi/","title":"MPI","text":""},{"location":"hpc/mpi/#librabric-providers","title":"Librabric providers","text":"<p><code>fi_info</code> is an utility program to query for available fabric interfaces.</p>"},{"location":"hpc/mpi/#cxi-provider","title":"CXI provider","text":"<p>The CXI provider enables libfabric on Cray's Slingshot network.</p> <p>The following checks if CXI is available:</p> <pre><code>fi_info -p cxi\n</code></pre> Check for CXI provider <p>CXI is available (for example by enabling the CXI hook in a container): <pre><code>$ fi_info -p cxi\nprovider: cxi\n    fabric: cxi\n    domain: cxi0\n    version: 0.1\n    type: FI_EP_RDM\n    protocol: FI_PROTO_CXI\n</code></pre></p> <p>CXI is not available: <pre><code>$ fi_info -p cxi\nfi_getinfo: -61 (No data available)\n</code></pre></p>"},{"location":"hpc/mpi/#mpi-environment-variables","title":"MPI Environment Variables","text":""},{"location":"hpc/mpi/#environment-variables-set-for-mpi-applications","title":"Environment variables set for MPI applications","text":"<p>It is sometimes useful to use environment variables related to MPI, for example to set a file name for each process.</p> MPICHOpenMPISlurm <p>The default <code>mpiexec</code> (<code>mpiexec.hydra</code>) sets the following environment variables for each process:</p> <ul> <li><code>PMI_RANK</code>: MPI rank</li> <li><code>PMI_SIZE</code>: tatal number of MPI processes</li> </ul> <p>OpenMPI provides the following environment variables for each process:</p> <ul> <li><code>OMPI_COMM_WORLD_RANK</code>: numbere of ranks in <code>MPI_COMM_WORLD</code></li> <li><code>OMPI_COMM_WORLD_SIZE</code>: rank of the current process in <code>MPI_COMM_WORLD</code></li> <li><code>OMPI_COMM_WORLD_LOCAL_RANK</code>: relative rank of the process on the node</li> <li><code>OMPI_COMM_WORLD_LOCAL_SIZE</code>: number of ranks on the node</li> </ul> <p>Slurm provides the following environment variables for each process:</p> <ul> <li><code>SLURM_NPROCS</code>: total number of processes</li> <li><code>SLURM_PROCID</code>: MPI rank of the current process</li> <li><code>SLURM_LOCALID</code>: node-local ID for the process</li> </ul> Naming nsys report files with MPI rank <pre><code>mpiexec -n 4 nsys profile --trace=cuda,mpi --mpi-impl=mpich -o report_rank%q{PMI_RANK} &lt;APPLICATION&gt;\n</code></pre> <p>The command above will create a report for each MPI rank.</p>"},{"location":"hpc/slurm/","title":"Slurm","text":"<p>Tip and tricks about the Slurm workload manager.</p>"},{"location":"hpc/slurm/#slurm-configuration","title":"Slurm configuration","text":""},{"location":"hpc/slurm/#check-generic-resources-gres","title":"Check generic resources (Gres)","text":"<p>The following command will show the generic resources (Gres) available on the node:</p> <pre><code>scontrol show nods | grep Gres | sort -u\n</code></pre> <p>This command can be used to check for misconfigures nodes.</p>"},{"location":"hpc/slurm/#check-reservations","title":"Check reservations","text":"<pre><code>scontrol show res\n</code></pre> <p>To run on a serervations use the <code>--reservation</code> flag:</p> <pre><code>srun --reservation=&lt;reservation_name&gt; &lt;command&gt;\n</code></pre>"},{"location":"linux/commands/","title":"Useful commands","text":""},{"location":"linux/commands/#kill-unresponsive-process","title":"Kill unresponsive process","text":"<p>An unresponsive processe/service ignores the <code>kill</code> command.  In order to shut down the process immediately use</p> <pre><code>kill -9 PID\n</code></pre> <p>where <code>PID</code> is the process ID.</p> <p>This is equivalent to</p> <pre><code>kill -SIGKILL PID\n</code></pre> <p>The <code>kill -SIGKILL</code> command bypasses the standard shutdown routine and unsaved data will be lost.</p>"},{"location":"linux/commands/#remove-broken-symbolic-links","title":"Remove broken symbolic links","text":"<pre><code>find &lt;DIR&gt; -xtype l -delete\n</code></pre>"},{"location":"linux/commands/#sycalls-id-to-name-mapping","title":"Sycalls ID to name mapping","text":"<p>The following command shows the mapping of syscall IDs to their names:</p> <pre><code>ausyscall --dump\n</code></pre> Installation <p><code>ausyscall</code> is part of the <code>audit</code> package.</p> <pre><code>sudo apt install auditd\n</code></pre>"},{"location":"linux/config/","title":"Configuration","text":""},{"location":"linux/config/#oem-installation","title":"OEM Installation","text":"<p>Since Ubuntu <code>24.04</code>, the OEM installer doesn't show up in the GRUB menu by default.</p> <p>From the live USB, you can install the OEM version by pressing using the following:</p> <pre><code>sudo mkdir -p /usr/share/desktop-provision\nsudo bash -c 'echo \"mode: oem\" &gt; /usr/share/desktop-provision/whitelabel.yaml' # (1)!\n/snap/bin/ubuntu-desktop-bootstrap --try-or-install # (2)!\n</code></pre> <ol> <li>Creates a configuration file that tells the installer to run in OEM mode.</li> <li>Runs the installer in OEM mode, allowing you to set up the system for end users.</li> </ol>"},{"location":"networking/ssh/","title":"Secure SHell protocol (SSH protocol)","text":""},{"location":"networking/ssh/#force-password","title":"Force password","text":"<p>If passwordless SSH connections fail with the following error</p> <pre><code>Too many authentication failures\n</code></pre> <p>it is possible to force SSH to ask for a password with the following options:</p> <pre><code>ssh -o PreferredAuthentications=password -o PubkeyAuthentication=no HOST\n</code></pre>"},{"location":"networking/ssh/#passwordless-access","title":"Passwordless access","text":""},{"location":"networking/ssh/#generate-privatepublic-keys-pair","title":"Generate private/public keys pair","text":"<pre><code>ssh-keygen -t rsa\n</code></pre>"},{"location":"networking/ssh/#add-public-key-to-remote-server","title":"Add public key to remote server","text":"<pre><code>ssh-copy-id -i ~/.ssh/KEY_FILE_NAME USER@HOST\n</code></pre>"},{"location":"programming/cpp/","title":"C++","text":""},{"location":"programming/cpp/#libraries","title":"Libraries","text":"<p>List of random interesting C/C++ libraries:</p> <ul> <li>cpptrace: simple, portable, and self-contained stacktrace library for C++11 and newer</li> <li>GSL: guidelines support library</li> <li>zdys: fast and lightweight x86/x86-64 disassembler and code generation library</li> </ul>"},{"location":"programming/cpp/#least-recently-used-cache-lru-cache","title":"Least recently used cache (LRU cache)","text":"<p>Python provides a decorator to wrap a function with a memoizing callable that saves up to the <code>maxsize</code> most recent calls:</p> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef foo():\n    pass\n</code></pre> <p>The Functional Programming in C++<sup>1</sup> book provides an example of general unbounded function cache (similar to Python's <code>@functools.cache</code>) implemented using <code>std::map</code> (\\(\\mathcal{O}(\\log n)\\))  insertion and retrieval with <code>operator[]</code>).</p> <p>Since insertion and retrieval of elements (using <code>operator[]</code>) is \\(\\mathcal{O}(\\log n)\\) for <code>std::map</code>, the implementation in is not the most efficient.  Additionally, <code>std::map</code> does not retain the order of insertion which is necessary to implement LRU cache.</p> <p>An LRU cache can be implemented efficiently using two data structure: </p> <ol> <li>a <code>std::deque</code> (ordered, with \\(\\mathcal{O}(1)\\) insertion or removal of elements at the end or beginning), and</li> <li>a <code>std::unordered_map</code> (average \\(\\mathcal{O}(1)\\) insertion and removal).</li> </ol> <p>The <code>std::deque</code> keeps track of the elements added to cache, and allows to easily and efficiently evict old elements when the cache is full and a new element needs to be stored. <code>std::unordered_map</code> provides an efficient mapping between the function arguments (stored as keys) and the position of the corresponding cached value (stored as values).</p> <pre><code>#include &lt;deque&gt;\n#include &lt;tuple&gt;\n#include &lt;utility&gt;\n#include &lt;unordered_map&gt;\n#include &lt;optional&gt;\n\ntemplate &lt;typename Result, typename... Args&gt;\nauto lru_cache(Result (*f)(Args...), std::optional&lt;size_t&gt; cache_size = std::nullopt) {\n    using cache_t = std::deque&lt;std::pair&lt;Args..., Result&gt;&gt;; \n    cache_t cache; // (1)!\n\n    std::unordered_map&lt;Args..., typename cache_t::iterator&gt; cache_map; // (2)!\n\n    return [f, cache, cache_map, cache_size](Args... args) mutable -&gt; Result { // (3)!\n        auto cached_map_it = cache_map.find(args...);\n\n        if(cached_map_it == cache_map.end()){ // (4)!\n            if(cache_size &amp;&amp; cache.size() == *cache_size){ // (5)! \n                // Remove oldest element from the cache\n                cache_map.erase(cache.front().first); // (6)!\n                cache.pop_front(); // (7)!\n            }\n            // Compute new element, add it to the cache, and return it\n            const auto result = f(args...);\n            cache.emplace_back(std::make_pair(args..., result)); // (8)!\n            cache_map.emplace(args..., cache.end() - 1); // (9)!\n            return result;\n        }\n        else{ // (10)!\n            // Update cached element position in the cache and return it\n            auto cached_it = cached_map_it-&gt;second; // (11)!\n            cache.push_back(*cached_it); // (12)!\n            cache.erase(cached_it); // (13)!\n            cached_map_it-&gt;second = cache.end() - 1; // (14)!\n\n            return cache.back().second;\n        }\n    };\n}\n</code></pre> <ol> <li><code>std::deque</code> stores a pair of arguments and the result of the function, and represents the cache.</li> <li><code>std::unordered_map</code> maps arguments to elements in the cache (<code>std::deque</code>) and provides \\(\\mathcal{O}(1)\\) access to the cache.</li> <li>The lambda is <code>mutable</code> in order to modify the cache and the cache map.</li> <li>The arguments are not found in the cache.</li> <li>The cache is full.</li> <li>Remove element from <code>cache_map</code> corresponding to the arguments of the oldest element in the cache (at the front of the queue).</li> <li>Remove the oldest element from the cache.</li> <li>Add pair of results and arguments to the cache.</li> <li>Add the arguments and the iterator to the last element in the cache to <code>cache_map</code>.    The <code>std::deque::end</code> iterator points after the last element in the <code>std::deque</code>.</li> <li>The arguments are found in the cache.</li> <li><code>std::unordered_map::find</code> returns an iterator to the element in the map.     An element of <code>std::unordered_map</code> is a <code>std::pair</code> with the key and the value.     The value is an iterator to the element in the <code>std::deque</code>.     <code>cached_map_it-&gt;second</code> is the iterator to the element in the <code>std::deque</code> (the cache).</li> <li>Add the requested cahed element at the back of the queue, it is now the most recently used element.</li> <li>Remove the cached element from the current position in the queue, since it is now in the back.</li> <li>Update the iterator in the <code>std::unordered_map</code> to point to the new position of the cached element, which is at the back of the queue.     <code>std::deque::end</code> points after the last element in the <code>std::deque</code>.</li> </ol> <p>The <code>lru_cache</code> function returns a lambda that wraps the function <code>f</code> and implements the LRU cache. It can be used as follows:</p> <pre><code>auto cached_f = lru_cache(f, 100);\n</code></pre> <ol> <li> <p>Functional Programming in C++, Ivan \u010cuki\u0107, Manning Publications Co., ISBN 9781617293818.\u00a0\u21a9</p> </li> </ol>"},{"location":"programming/fortran/","title":"Fortran","text":"<p>Additional code snippets and notes can be found at RMeli/fortran-playground.</p>"},{"location":"programming/fortran/#scalapack","title":"ScaLAPACK","text":""},{"location":"programming/fortran/#numroc","title":"<code>numroc</code>","text":"<p><code>numroc</code> computes the NUMber of Rows Or Columns of a distributed matrix owned by the process indicated by <code>iproc</code>. Trying to use <code>numroc</code> in a Fortran program, results in the following error:</p> <pre><code>   61 |     m = numroc(n, 1, myrow, 0, nprow)\n      |        1\nError: Function 'numroc' at (1) has no IMPLICIT type\n</code></pre> <p>In order to avoid this error, the function needs to be declared as external.</p> <pre><code>integer, external :: numroc\n</code></pre> <code>external</code> statement <p>The <code>external</code> statement specifies procedures or dummy procedures as external, and allows their symbolic names to be used as actual arguments.</p>"},{"location":"programming/fortran/#busy-wait","title":"Busy Wait","text":"<pre><code>subroutine busywait(wait_time)\n   real(kind=dp), intent(in) :: wait_time ! (1)!\n\n   real(kind=dp) :: t_start, t_now, elapsed_time\n\n   elapsed_time = 0.0_dp\n\n   call cpu_time(t_start)\n\n   do while (elapsed_time &lt; wait_time)\n      call cpu_time(t_now)\n      elapsed_time = t_now - t_start\n   end do\n\nend subroutine\n</code></pre> <ol> <li>Time to wait in seconds.</li> </ol>"},{"location":"python/conda/","title":"<code>conda</code>","text":""},{"location":"python/conda/#move-conda-directory","title":"Move <code>.conda</code> Directory","text":""},{"location":"python/conda/#problem","title":"Problem","text":"<p>The <code>.conda</code> directory might get quite beefy over time. This might cause problems with disk space (for example on the <code>$HOME</code> directory on an HPC cluster) and will need to be moved.</p>"},{"location":"python/conda/#solution","title":"Solution","text":"<p>Move the <code>.conda</code> directory as usual:</p> <pre><code>mv $SOURCE/.conda $DESTINATION/.conda\n</code></pre> <p>Amend <code>conda</code> configuration:</p> <pre><code>conda config --remove pkgs_dirs $SOURCE/.conda/pkgs\nconda config --add pkgs_dirs $DESTINATION/.conda/pkgs\nconda config --remove envs_dirs $SOURCE/.conda/envs\nconda config --add envs_dirs $DESTINATION/.conda/envs\n</code></pre> <p>Restart the shell. Everything should be good to go.</p>"},{"location":"python/conda/#using-conda-with-slurm","title":"Using <code>conda</code> with SLURM","text":""},{"location":"python/conda/#problem_1","title":"Problem","text":"<p>Many HPC clusters provide modules for Anaconda. However, the command <code>conda activate ENV</code> within a SLURM script fails with the following error:</p> <pre><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n</code></pre> <p>The problem is that the (user-specific) <code>.condarc</code> configuration file is not being loaded when running a SLURM script with <code>sbatch</code>.</p>"},{"location":"python/conda/#solution_1","title":"Solution","text":"<p>Use</p> <pre><code>#!/bin/bash -l\n</code></pre> <p>instead of <code>#!/bin/bash</code> at the top of the SLURM script. </p> <p>The <code>-l</code> option forces <code>bash</code> to act as a login shell (thus loading the user-specific <code>.condarc</code> file) instead of acting as a blank shell.</p>"},{"location":"python/mdanalysis/","title":"MDAnalysis","text":""},{"location":"python/mdanalysis/#openmp-on-macos","title":"OpenMP on macOS","text":"<p>MDanalysis does not compile with OpenMP support on macOS.</p>"},{"location":"python/mdanalysis/#why","title":"Why?","text":"<p>When building, MDanalysis checks if the following program can be compiled with the current compiler:</p> <pre><code>#include &lt;omp.h&gt;\n\nint main(void) {\n    omp_get_num_threads();\n}\n</code></pre> <p>On macOS, compiling such program with <code>g++ -fopenmp</code> fails with the following error:</p> <pre><code>clang: error: unsupported option '-fopenmp'\nclang: error: unsupported option '-fopenmp'\n</code></pre> <p><code>g++</code> on macOS is an alias for <code>clang++</code>.</p>"},{"location":"python/mdanalysis/#openmp-support-with-gcc","title":"OpenMP Support with GCC","text":"<p>Install GCC using <code>brew install gcc</code> (or any other package manager).</p> <pre><code>CC=gcc-11 CXX=g++-11 pip install -e package\n</code></pre>"},{"location":"python/mdanalysis/#disabling-openmp-support","title":"Disabling OpenMP Support","text":"<p>OpenMP support can be explicitly disabled with</p> <pre><code>MDA_USE_OPENMP=FALSE CC=gcc-11 CXX=g++-11 pip install -e package\n</code></pre>"},{"location":"tools/rsync/","title":"<code>rsync</code>","text":"<p><code>rsync</code> is an open source utility that provides fast incremental file transfer.</p>"},{"location":"tools/rsync/#sync-remote-molder","title":"Sync remote molder","text":"<p>Sync a remote folder locally:</p> <pre><code>rsync -avzhP user@host:HOST_PATH LOCAL_PATH\n</code></pre> <p><code>-a</code> enables archive mode (same as <code>-rlptgoD</code> without <code>-H</code>), <code>-v</code> increases verbosity, <code>-h</code> outputs numbers in a human-readable format, <code>-z</code> compresses file data during the transfer, and <code>-P</code> keeps partially transferred files and shows progress during the transfer.</p>"}]}